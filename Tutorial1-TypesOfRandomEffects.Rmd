---
title: "Fun with merMod Objects"
author: "Jared E. Knowles"
date: "Saturday, April 26, 2014"
output: html_document
---

```{r setup, echo=FALSE, error=FALSE, message=FALSE, eval=TRUE, results='hide'}
opts_chunk$set(dev='svg', fig.width=6, fig.height=6, echo=TRUE, 
               message=FALSE, error=FALSE, warning=FALSE)

```


# Introduction

First of all, be warned, the terminology surrounding multilevel models is vastly 
inconsistent. For example, multilevel models themselves may be referred to as 
hierarchical linear models, random effects models, multilevel models, random 
intercept models, random slope models, or pooling models. Depending on the 
discipline, software used, and the academic literature many of these terms may 
be referring to the same general modeling strategy. In this tutorial I will 
attempt to provide a user guide to multilevel modeling by demonstrating how 
to fit multilevel models in R and by attempting to connect the model fitting 
procedure to commonly used terminology used regarding these models. 

We will cover the following topics: 

- The structure and methods of `merMod` objects
- Extracting random effects of `merMod` objects
- Plotting and interpreting `merMod` objects

If you haven't already, make sure you head over to the [Getting Started With 
Multilevel Models tutorial](http://jaredknowles.com/journal/2013/11/25/getting-started-with-mixed-effect-models-in-r)
in order to ensure you have set up your environment correctly and installed all 
the necessary packages. The tl;dr is that you will need:

- A current version of R (2.15 or greater)
- The `lme4` package (`install.packages("lme4")`)

# Read in the data

Multilevel models are appropriate for a particular kind of data structure where 
units are nested within groups (generally 5+ groups) and where we want to model 
the group structure of the data. For our introductory example we will start with 
a simple example from the `lme4` documentation and explain what the model is doing. 
We will use data from Jon Starkweather at the [University of North Texas](http://www.unt.edu/rss/class/Jon/R_SC/). Visit the excellent tutorial [available here for more.](http://www.unt.edu/rss/class/Jon/R_SC/Module9/LMM_Examples.R)

```{r loadandviewdata}
library(lme4) # load library
library(arm) # convenience functions for regression in R
lmm.data <- read.table("http://www.unt.edu/rss/class/Jon/R_SC/Module9/lmm.data.txt",
   header=TRUE, sep=",", na.strings="NA", dec=".", strip.white=TRUE)
#summary(lmm.data)
head(lmm.data)
```

Here we have data on the extroversion of subjects nested within classes and within 
schools. 

Let's understand the structure of the data a bit before we begin:

```{r explore}
str(lmm.data)
```

Here we see we have two possible grouping variables -- `class` and `school`. Let's 
explore them a bit further:

```{r exploregroups}
table(lmm.data$class)
table(lmm.data$school)
table(lmm.data$class, lmm.data$school)
```

This is a perfectly balanced dataset. In all likelihood you aren't working with 
a perfectly balanced dataset, but we'll explore the implications for that in 
the future. For now, let's plot the data a bit. Using the excellent `xyplot` 
function in the `lattice` package, we can explore the relationship between 
schools and classes across our variables. 

```{r xyplot1}
require(lattice)
xyplot(extro ~ open + social + agree | class, data = lmm.data, 
                 auto.key = list(x = .85, y = .035, corner = c(0, 0)), 
       layout = c(4,1), main = "Extroversion by Class") 
```

Here we see that within classes there are clear stratifications and we also 
see that the `social` variable is strongly distinct from the `open` and `agree` 
variables. We also see that class `a` and class `d` have significantly more spread 
in their lowest and highest bands respectively. Let's next plot the data by 
`school`. 


```{r xyplot2}
xyplot(extro ~ open + social + agree | school, data = lmm.data, 
                 auto.key = list(x = .85, y = .035, corner = c(0, 0)), 
       layout = c(3, 2), main = "Extroversion by School") 
```

By school we see that students are tightly grouped, but that school `I` and school 
`VI` show substantially more dispersion than the other schools. The same pattern 
among our predictors holds between schools as it did between classes. Let's put 
it all together:


```{r xyplot3}
xyplot(extro ~ open + social + agree | school + class, data = lmm.data, 
                 auto.key = list(x = .85, y = .035, corner = c(0, 0)), 
       main = "Extroversion by School and Class") 
```

Here we can see that school and class seem to closely differentiate the 
relationship between our predictors and extroversion. 


# Fit and evaluate a random intercept model

In the last tutorial we fit a series of random intercept models to our nested data. 
We will examine the `lmerMod` object produced when we fit this model in much 
more depth in order to understand how to work with mixed effect models in R. We 
start by fitting a the basic example below grouped by class:

```{r lmer1}
MLexamp1 <- lmer(extro ~ open + agree + social + (1|school), data=lmm.data)
class(MLexamp1)
```

First, we see that `MLexamp1` is now an R object of the class `lmerMod`. This 
`lmerMod` object is an **S4** class, and to explore its structure, we use `slotNames`:

```{r lmermod}
slotNames(MLexamp1)
#showMethods(classes="lmerMod")
```

Within the `lmerMod` object we see a number of objects that we may wish to explore. 
To look at any of these, we can simply type `MLexamp1@` and then the slot name 
itself. For example:

```{r slotnames1}
MLexamp1@call # returns the model call
MLexamp1@beta # returns the betas
class(MLexamp1@frame) # returns the class for the frame slot
head(MLexamp1@frame) # returns the model frame
```

The `merMod` object has a number of methods available -- too many to enumerate 
here. But, we will go over some of the more common in the list below:


```{r modMethods}
methods(class = "merMod")

```

A common need is to extract the fixed effects from a `merMod` object. `fixef` 
extracts a named numeric vector of the fixed effects, which is handy. 

```{r fixef}
fixef(MLexamp1)

```

Another common need is to extract the residual standard error, which is necessary 
for calculating effect sizes. To get a named vector of the residual standard 
error:

```{r sigmaef}
sigma(MLexamp1)
```

For example, it is common practice in education research to standardize fixed 
effects into "effect sizes" by dividing the fixed effect paramters by the residual 
standard error, which can be accomplished in `lme4` easily:

```{r effsize}
fixef(MLexamp1) / sigma(MLexamp1)

```

From this, we can see that our predictors of openness, agreeableness and social 
are virtually useless in predicting extroversion. Let's turn our attention to the 
random effects next. 

## Explore Group Variation and Random Effects

In all likelihood you fit a mixed-effect model because you are directly interested 
in the group-level variation in your model. It is not immediately clear how to 
explore this group level variation from the results of `summary.merMod`. What 
we get from this output is the variance and the standard deviation of the group 
effect, but we do not get effects for individual groups. This is where the `ranef` 
function comes in handy. 

```{r ranef1}
ranef(MLexamp1)

```

Running the `ranef` function gives us the intercepts for each school, but not much 
additional information -- for example the precision of these estimates. To do that, 
we need some additional commands:

```{r ranef2}
re1 <- ranef(MLexamp1, condVar=TRUE) # save the ranef.mer object
class(re1)
attr(re1[[1]], which = "postVar")

```

The `ranef.mer` object is a list which contains a data.frame for each group level. 
The dataframe contains the random effects for each group (here we only have an 
intercept for each school). When we ask `lme4` for the conditional variance of 
the random effects it is stored in an `attribute` of those dataframes as a list 
of variance-covariance matrices. 

This structure is indeed *complicated*, but it is powerful as it allows for nested, 
grouped, and cross-level random effects. Also, the creators of `lme4` have provided 
users with some simple shortcuts to get what they are really interested in out of 
a `ranef.mer` object. 

```{r ranef3}
re1 <- ranef(MLexamp1, condVar=TRUE, whichel = "school")
print(re1)
dotplot(re1)
```

## Non-nested Classes

```{r lmer2}
MLexamp.7 <- lmer(extro ~ open + agree + social + (1|school) + (1|class), data=lmm.data)
display(MLexamp.7)
```

## Nested group effects

And finally, we can fit nested group effect terms through the following syntax:

```{r lmer3}
MLexamp.8 <- lmer(extro ~ open + agree + social + (1|school/class), data=lmm.data)
display(MLexamp.8)


```

Here the `(1|school/class)` says that we want to fit a mixed effect term for varying 
intercepts `1|` by schools, and for classes that are nested within schools. 


## Non-correlated random effects

```{r lmer4}
MLexampUnCor <- lmer(extro ~ open + agree + social + (1|school/class) + (0 + open|school/class), 
                     data = lmm.data)
MLexampCor <- lmer(extro ~ open + agree + social + (1+open|school/class),
                     data = lmm.data)

VarCorr(MLexampUnCor)
VarCorr(MLexampCor)
```
